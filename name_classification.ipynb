{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4b2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device) \n",
    "print(f\"using device {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8d5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import unicodedata \n",
    "\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters) \n",
    "\n",
    "def unicodeToAscii(s): \n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d14b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting 'Ślusàrski' to Slusarski\n"
     ]
    }
   ],
   "source": [
    "print (f\"converting 'Ślusàrski' to {unicodeToAscii('Ślusàrski')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80242015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    if letter not in allowed_characters: \n",
    "        return allowed_characters.find('_')\n",
    "    else: \n",
    "        return allowed_characters.find(letter) \n",
    "\n",
    "def lineToTensor(line): \n",
    "    tensor = torch.zeros(1, len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[0][li][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eeb86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0')\n",
      "The name 'Ahn' becomes tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (f\"The letter 'a' becomes {lineToTensor('a')}\") #notice that the first position in the tensor = 1\n",
    "print (f\"The name 'Ahn' becomes {lineToTensor('Ahn')}\") #notice 'A' sets the 27th index to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e54e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open \n",
    "import glob \n",
    "import os \n",
    "import time \n",
    "\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "class NamesDataset(Dataset): \n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir \n",
    "        self.load_time = time.localtime \n",
    "        labels_set = set()\n",
    "\n",
    "        self.data = [] \n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in text_files: \n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label) \n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines: \n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for label in self.labels:\n",
    "            temp_tensor = torch.tensor(self.labels_uniq.index(label))\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        label_item = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "            \n",
    "        return data_item, label_item, data_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99153f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 20074 items of data\n",
      "example = ('Khoury', 'Arabic', tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]], device='cuda:0'), tensor(8, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "alldata = NamesDataset(\"data/names/\")\n",
    "print(f\"loaded {len(alldata)} items of data\")\n",
    "print(f\"example = {alldata[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1a6a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples = 17063, validation examples = 3011\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(alldata, [0.85, 0.15], generator=torch.Generator(device=device).manual_seed(2026))\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ee4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        out = self.h2o(hidden[0])\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6460e015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (rnn): RNN(58, 128, batch_first=True)\n",
      "  (h2o): Linear(in_features=128, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 128\n",
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2cec85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9745, -2.7938, -2.7299, -2.8898, -2.9408, -2.9859, -3.0303, -2.8996,\n",
      "         -2.7709, -2.7795, -2.9623, -2.9458, -3.0190, -2.8303, -2.9887, -2.9269,\n",
      "         -3.0164, -2.6509]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)\n",
      "('English', 17)\n"
     ]
    }
   ],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i \n",
    "\n",
    "input = lineToTensor('Albert')\n",
    "output = rnn(input) #this is equivalent to ``output = rnn.forward(input)``\n",
    "print(output)\n",
    "print(label_from_output(output, alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aaaf120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "\n",
    "def train(rnn, training_data, epochs = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
    "\n",
    "    current_loss = 0 \n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Training on data set with {len(training_data)} samples\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        rnn.zero_grad()\n",
    "\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches) \n",
    "        batches = np.array_split(batches, len(batches)//n_batch_size)\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (data_item, label_item, data_tensor, label_tensor) = training_data[i]\n",
    "                output = rnn.forward(data_tensor)\n",
    "                loss = criterion(output.squeeze(), label_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if epoch % report_every == 0:\n",
    "            print(f\"{epoch} ({epoch / epochs:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20e120f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on data set with 17063 samples\n",
      "0 (0%): \t average batch loss = 1.3862715135529387\n",
      "5 (19%): \t average batch loss = 0.8394300906850535\n",
      "10 (37%): \t average batch loss = 0.6712975169396801\n",
      "15 (56%): \t average batch loss = 0.5645289322995878\n",
      "20 (74%): \t average batch loss = 0.49029476373974057\n",
      "25 (93%): \t average batch loss = 0.43026199103944424\n",
      "training took 1698.2123827934265s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_losses = train(rnn, train_set, epochs=27, learning_rate=0.15, report_every=5)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "759f2d4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def evaluate(rnn, testing_data, classes):\n",
    "    confusion = torch.zeros(len(classes), len(classes))\n",
    "\n",
    "    rnn.eval() #set to eval mode\n",
    "    with torch.no_grad(): # do not record the gradients during eval phase\n",
    "        for i in range(len(testing_data)):\n",
    "            (data_item, label_item, data_tensor, label_tensor) = testing_data[i]\n",
    "            output = rnn(data_tensor)\n",
    "            guess, guess_i = label_from_output(output, classes)\n",
    "            label_i = classes.index(label_item)\n",
    "            confusion[label_i][guess_i] += 1\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(len(classes)):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # sphinx_gallery_thumbnail_number = 2\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "evaluate(rnn, test_set, classes=alldata.labels_uniq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb23a3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0x300: '̀' - COMBINING GRAVE ACCENT\n",
      "0x301: '́' - COMBINING ACUTE ACCENT\n",
      "0x302: '̂' - COMBINING CIRCUMFLEX ACCENT\n",
      "0x303: '̃' - COMBINING TILDE\n",
      "0x304: '̄' - COMBINING MACRON\n",
      "0x305: '̅' - COMBINING OVERLINE\n",
      "0x306: '̆' - COMBINING BREVE\n",
      "0x307: '̇' - COMBINING DOT ABOVE\n",
      "0x308: '̈' - COMBINING DIAERESIS\n",
      "0x309: '̉' - COMBINING HOOK ABOVE\n",
      "0x30a: '̊' - COMBINING RING ABOVE\n",
      "0x30b: '̋' - COMBINING DOUBLE ACUTE ACCENT\n",
      "0x30c: '̌' - COMBINING CARON\n",
      "0x30d: '̍' - COMBINING VERTICAL LINE ABOVE\n",
      "0x30e: '̎' - COMBINING DOUBLE VERTICAL LINE ABOVE\n",
      "0x30f: '̏' - COMBINING DOUBLE GRAVE ACCENT\n",
      "0x310: '̐' - COMBINING CANDRABINDU\n",
      "0x311: '̑' - COMBINING INVERTED BREVE\n",
      "0x312: '̒' - COMBINING TURNED COMMA ABOVE\n",
      "0x313: '̓' - COMBINING COMMA ABOVE\n",
      "0x314: '̔' - COMBINING REVERSED COMMA ABOVE\n",
      "0x315: '̕' - COMBINING COMMA ABOVE RIGHT\n",
      "0x316: '̖' - COMBINING GRAVE ACCENT BELOW\n",
      "0x317: '̗' - COMBINING ACUTE ACCENT BELOW\n",
      "0x318: '̘' - COMBINING LEFT TACK BELOW\n",
      "0x319: '̙' - COMBINING RIGHT TACK BELOW\n",
      "0x31a: '̚' - COMBINING LEFT ANGLE ABOVE\n",
      "0x31b: '̛' - COMBINING HORN\n",
      "0x31c: '̜' - COMBINING LEFT HALF RING BELOW\n",
      "0x31d: '̝' - COMBINING UP TACK BELOW\n",
      "0x31e: '̞' - COMBINING DOWN TACK BELOW\n",
      "0x31f: '̟' - COMBINING PLUS SIGN BELOW\n",
      "0x320: '̠' - COMBINING MINUS SIGN BELOW\n",
      "0x321: '̡' - COMBINING PALATALIZED HOOK BELOW\n",
      "0x322: '̢' - COMBINING RETROFLEX HOOK BELOW\n",
      "0x323: '̣' - COMBINING DOT BELOW\n",
      "0x324: '̤' - COMBINING DIAERESIS BELOW\n",
      "0x325: '̥' - COMBINING RING BELOW\n",
      "0x326: '̦' - COMBINING COMMA BELOW\n",
      "0x327: '̧' - COMBINING CEDILLA\n",
      "0x328: '̨' - COMBINING OGONEK\n",
      "0x329: '̩' - COMBINING VERTICAL LINE BELOW\n",
      "0x32a: '̪' - COMBINING BRIDGE BELOW\n",
      "0x32b: '̫' - COMBINING INVERTED DOUBLE ARCH BELOW\n",
      "0x32c: '̬' - COMBINING CARON BELOW\n",
      "0x32d: '̭' - COMBINING CIRCUMFLEX ACCENT BELOW\n",
      "0x32e: '̮' - COMBINING BREVE BELOW\n",
      "0x32f: '̯' - COMBINING INVERTED BREVE BELOW\n",
      "0x330: '̰' - COMBINING TILDE BELOW\n",
      "0x331: '̱' - COMBINING MACRON BELOW\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "mn_chars = []\n",
    "for i in range(0x10000):  # Basic Multilingual Plane\n",
    "    char = chr(i)\n",
    "    if unicodedata.category(char) == 'Mn':\n",
    "        mn_chars.append((hex(i), char, unicodedata.name(char, 'UNNAMED')))\n",
    "\n",
    "# Show first 50\n",
    "for code, char, name in mn_chars[:50]:\n",
    "    print(f\"{code}: '{char}' - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb02e941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
