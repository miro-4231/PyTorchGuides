{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83a4b0835b026d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.515677800Z",
     "start_time": "2026-02-02T09:13:46.492344300Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "756847a90d767f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.536210Z",
     "start_time": "2026-02-02T09:13:48.526679700Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0 \n",
    "EOS_token = 1 \n",
    "\n",
    "class Lang: \n",
    "    def __init__(self, name): \n",
    "        self.name = name \n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx: \n",
    "            self.word2idx[word] = self.n_words \n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word \n",
    "            self.n_words += 1 \n",
    "        else: \n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74c09a60d48d525d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.553335400Z",
     "start_time": "2026-02-02T09:13:48.537227500Z"
    }
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) \n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eee82734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.562272100Z",
     "start_time": "2026-02-02T09:13:48.554336Z"
    }
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading Lines...\") \n",
    "\n",
    "    lines = open('data/%s-%s.txt'% (lang1, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse: \n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else: \n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "741e710f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.571349Z",
     "start_time": "2026-02-02T09:13:48.563269400Z"
    }
   },
   "outputs": [],
   "source": [
    "Max_length = 10 \n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p): \n",
    "    return len(p[0].split(' ')) < Max_length and\\\n",
    "        len(p[1].split(' ')) < Max_length and\\\n",
    "        p[1].startswith(eng_prefixes) \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fda3bf93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.137745200Z",
     "start_time": "2026-02-02T09:13:48.572348600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['je ne suis pas instituteur', 'i m not a teacher']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False): \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17d490ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.174485500Z",
     "start_time": "2026-02-02T09:13:52.165789300Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1): \n",
    "        super(EncoderRNN, self).__init__() \n",
    "        self.hidden_size = hidden_size \n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True) \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input): \n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        output, hidden = self.gru(embedded) \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "715eedc1cc17e0a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:14:14.859618500Z",
     "start_time": "2026-02-02T09:14:14.812310500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = next(enc.parameters()).device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "250ecab3d117cf47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:26:12.047024400Z",
     "start_time": "2026-02-02T09:26:12.008665700Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(Max_length):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c17d6fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.196908700Z",
     "start_time": "2026-02-02T09:13:52.174485500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 24, 30])\n",
      "torch.Size([1, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "enc = EncoderRNN(20, 30)\n",
    "with torch.no_grad():\n",
    "    out = enc(torch.randint(high=13,size=(10,24)).to(dtype=torch.long))\n",
    "    print(out[0].shape)\n",
    "    print(out[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7a0e7b2c2d339a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:27:52.628780400Z",
     "start_time": "2026-02-02T09:27:52.565290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 25])\n",
      "torch.Size([1, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "dec = DecoderRNN(30, 25)\n",
    "with torch.no_grad():\n",
    "    out_ = dec(out[0], out[1], torch.randint(high=13,size=(10,24)).to(dtype=torch.long))\n",
    "    print(out_[0].shape)\n",
    "    print(out_[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ebd086aa7cabd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, Max_length), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, Max_length), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        # inp_ids.append(EOS_token)\n",
    "        # tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0135231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                criterion): \n",
    "\n",
    "    total_loss = 0.0 \n",
    "    for data in dataloader: \n",
    "        input_tensor, target_tensor = data \n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_ouputs, encoder_hidden = encoder(input_tensor) \n",
    "        decoder_outputs, _, _ = decoder(encoder_ouputs, encoder_hidden, target_tensor) \n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward() \n",
    "\n",
    "        encoder_optimizer.step() \n",
    "        decoder_optimizer.step() \n",
    "\n",
    "        total_loss += loss.item() \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df42da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math \n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent): \n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "586cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b67b7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f169128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang:Lang, output_lang:Lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "59843faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb4d32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "1m 12s (- 18m 11s) (5 6%) 1.7262\n",
      "2m 24s (- 16m 50s) (10 12%) 0.9719\n",
      "3m 29s (- 15m 7s) (15 18%) 0.6527\n",
      "4m 23s (- 13m 11s) (20 25%) 0.4570\n",
      "5m 23s (- 11m 51s) (25 31%) 0.3284\n",
      "6m 36s (- 11m 1s) (30 37%) 0.2412\n",
      "7m 32s (- 9m 41s) (35 43%) 0.1795\n",
      "8m 38s (- 8m 38s) (40 50%) 0.1379\n",
      "9m 39s (- 7m 31s) (45 56%) 0.1085\n",
      "10m 40s (- 6m 24s) (50 62%) 0.0875\n",
      "11m 48s (- 5m 22s) (55 68%) 0.0724\n",
      "12m 43s (- 4m 14s) (60 75%) 0.0626\n",
      "13m 32s (- 3m 7s) (65 81%) 0.0555\n",
      "14m 23s (- 2m 3s) (70 87%) 0.0484\n",
      "15m 14s (- 1m 0s) (75 93%) 0.0459\n",
      "16m 7s (- 0m 0s) (80 100%) 0.0421\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0ba18a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> j ai tres sommeil\n",
      "= i m very sleepy\n",
      "< i m very sleepy today <EOS>\n",
      "\n",
      "> je suis surpris\n",
      "= i m surprised\n",
      "< i m surprised to do it the remember <EOS>\n",
      "\n",
      "> vous etes fort grossieres\n",
      "= you re very rude\n",
      "< you re impressed aren t you ? <EOS>\n",
      "\n",
      "> il va vous conduire a l aeroport\n",
      "= he is going to drive you to the airport\n",
      "< he is going to drive you to the airport <EOS>\n",
      "\n",
      "> nous sommes tous occupes\n",
      "= we re all busy\n",
      "< we re all busy right now <EOS>\n",
      "\n",
      "> vous etes tres riches\n",
      "= you are very rich\n",
      "< you are not very good looking for you <EOS>\n",
      "\n",
      "> il est responsable de cet accident\n",
      "= he is responsible for the accident\n",
      "< he is responsible for his three s different <EOS>\n",
      "\n",
      "> vous n etes pas malade\n",
      "= you re not sick\n",
      "< you re not very good at you <EOS>\n",
      "\n",
      "> je suis touriste\n",
      "= i am a tourist\n",
      "< i m a little dizzy <EOS>\n",
      "\n",
      "> c est toi le plus vieux\n",
      "= you re the oldest\n",
      "< you re the only one who can help me <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a563a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
