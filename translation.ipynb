{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332fa426",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83a4b0835b026d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.515677800Z",
     "start_time": "2026-02-02T09:13:46.492344300Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from src.models import AttnDecoderRNN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756847a90d767f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.536210Z",
     "start_time": "2026-02-02T09:13:48.526679700Z"
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0 \n",
    "EOS_token = 1 \n",
    "\n",
    "class Lang: \n",
    "    def __init__(self, name): \n",
    "        self.name = name \n",
    "        self.word2idx = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1:\"EOS\"}\n",
    "        self.n_words = 2 \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2idx: \n",
    "            self.word2idx[word] = self.n_words \n",
    "            self.word2count[word] = 1 \n",
    "            self.index2word[self.n_words] = word \n",
    "            self.n_words += 1 \n",
    "        else: \n",
    "            self.word2count[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c09a60d48d525d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.553335400Z",
     "start_time": "2026-02-02T09:13:48.537227500Z"
    }
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip()) \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) \n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eee82734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.562272100Z",
     "start_time": "2026-02-02T09:13:48.554336Z"
    }
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading Lines...\") \n",
    "\n",
    "    lines = open('data/%s-%s.txt'% (lang1, lang2), encoding=\"utf-8\").\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    if reverse: \n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else: \n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741e710f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:48.571349Z",
     "start_time": "2026-02-02T09:13:48.563269400Z"
    }
   },
   "outputs": [],
   "source": [
    "Max_length = 10 \n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p): \n",
    "    return len(p[0].split(' ')) < Max_length and\\\n",
    "        len(p[1].split(' ')) < Max_length and\\\n",
    "        p[1].startswith(eng_prefixes) \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda3bf93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.137745200Z",
     "start_time": "2026-02-02T09:13:48.572348600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "['nous sommes deprimes', 'we re depressed']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False): \n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d490ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.174485500Z",
     "start_time": "2026-02-02T09:13:52.165789300Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module): \n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1): \n",
    "        super(EncoderRNN, self).__init__() \n",
    "        self.hidden_size = hidden_size \n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True) \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input): \n",
    "        embedded = self.dropout(self.embedding(input)) \n",
    "        output, hidden = self.gru(embedded) \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715eedc1cc17e0a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:14:14.859618500Z",
     "start_time": "2026-02-02T09:14:14.812310500Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m device = \u001b[38;5;28mnext\u001b[39m(\u001b[43menc\u001b[49m.parameters()).device\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'enc' is not defined"
     ]
    }
   ],
   "source": [
    "device = next(enc.parameters()).device\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ecab3d117cf47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:26:12.047024400Z",
     "start_time": "2026-02-02T09:26:12.008665700Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(Max_length):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c17d6fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:13:52.196908700Z",
     "start_time": "2026-02-02T09:13:52.174485500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 24, 30])\n",
      "torch.Size([1, 10, 30])\n"
     ]
    }
   ],
   "source": [
    "enc = EncoderRNN(20, 30)\n",
    "with torch.no_grad():\n",
    "    out = enc(torch.randint(high=13,size=(10,24)).to(dtype=torch.long))\n",
    "    print(out[0].shape)\n",
    "    print(out[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7a0e7b2c2d339a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T09:27:52.628780400Z",
     "start_time": "2026-02-02T09:27:52.565290Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecoderRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dec = \u001b[43mDecoderRNN\u001b[49m(\u001b[32m30\u001b[39m, \u001b[32m25\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      3\u001b[39m     out_ = dec(out[\u001b[32m0\u001b[39m], out[\u001b[32m1\u001b[39m], torch.randint(high=\u001b[32m13\u001b[39m,size=(\u001b[32m10\u001b[39m,\u001b[32m24\u001b[39m)).to(dtype=torch.long))\n",
      "\u001b[31mNameError\u001b[39m: name 'DecoderRNN' is not defined"
     ]
    }
   ],
   "source": [
    "dec = DecoderRNN(30, 25)\n",
    "with torch.no_grad():\n",
    "    out_ = dec(out[0], out[1], torch.randint(high=13,size=(10,24)).to(dtype=torch.long))\n",
    "    print(out_[0].shape)\n",
    "    print(out_[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ebd086aa7cabd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, Max_length), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, Max_length), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        # inp_ids.append(EOS_token)\n",
    "        # tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0135231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                criterion): \n",
    "\n",
    "    total_loss = 0.0 \n",
    "    for data in dataloader: \n",
    "        input_tensor, target_tensor = data \n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_ouputs, encoder_hidden = encoder(input_tensor) \n",
    "        decoder_outputs, _, _ = decoder(encoder_ouputs, encoder_hidden, target_tensor) \n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward() \n",
    "\n",
    "        encoder_optimizer.step() \n",
    "        decoder_optimizer.step() \n",
    "\n",
    "        total_loss += loss.item() \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df42da23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import math \n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent): \n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b67b7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f169128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang:Lang, output_lang:Lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59843faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d32bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "1m 12s (- 18m 11s) (5 6%) 1.7262\n",
      "2m 24s (- 16m 50s) (10 12%) 0.9719\n",
      "3m 29s (- 15m 7s) (15 18%) 0.6527\n",
      "4m 23s (- 13m 11s) (20 25%) 0.4570\n",
      "5m 23s (- 11m 51s) (25 31%) 0.3284\n",
      "6m 36s (- 11m 1s) (30 37%) 0.2412\n",
      "7m 32s (- 9m 41s) (35 43%) 0.1795\n",
      "8m 38s (- 8m 38s) (40 50%) 0.1379\n",
      "9m 39s (- 7m 31s) (45 56%) 0.1085\n",
      "10m 40s (- 6m 24s) (50 62%) 0.0875\n",
      "11m 48s (- 5m 22s) (55 68%) 0.0724\n",
      "12m 43s (- 4m 14s) (60 75%) 0.0626\n",
      "13m 32s (- 3m 7s) (65 81%) 0.0555\n",
      "14m 23s (- 2m 3s) (70 87%) 0.0484\n",
      "15m 14s (- 1m 0s) (75 93%) 0.0459\n",
      "16m 7s (- 0m 0s) (80 100%) 0.0421\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0ba18a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> j ai tres sommeil\n",
      "= i m very sleepy\n",
      "< i m very sleepy today <EOS>\n",
      "\n",
      "> je suis surpris\n",
      "= i m surprised\n",
      "< i m surprised to do it the remember <EOS>\n",
      "\n",
      "> vous etes fort grossieres\n",
      "= you re very rude\n",
      "< you re impressed aren t you ? <EOS>\n",
      "\n",
      "> il va vous conduire a l aeroport\n",
      "= he is going to drive you to the airport\n",
      "< he is going to drive you to the airport <EOS>\n",
      "\n",
      "> nous sommes tous occupes\n",
      "= we re all busy\n",
      "< we re all busy right now <EOS>\n",
      "\n",
      "> vous etes tres riches\n",
      "= you are very rich\n",
      "< you are not very good looking for you <EOS>\n",
      "\n",
      "> il est responsable de cet accident\n",
      "= he is responsible for the accident\n",
      "< he is responsible for his three s different <EOS>\n",
      "\n",
      "> vous n etes pas malade\n",
      "= you re not sick\n",
      "< you re not very good at you <EOS>\n",
      "\n",
      "> je suis touriste\n",
      "= i am a tourist\n",
      "< i m a little dizzy <EOS>\n",
      "\n",
      "> c est toi le plus vieux\n",
      "= you re the oldest\n",
      "< you re the only one who can help me <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587a563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 4601\n",
      "eng 2991\n",
      "0m 47s (- 15m 0s) (5 5%) 1.5662\n",
      "1m 31s (- 13m 42s) (10 10%) 0.7109\n",
      "2m 17s (- 13m 1s) (15 15%) 0.3684\n",
      "3m 0s (- 12m 1s) (20 20%) 0.2025\n",
      "3m 46s (- 11m 19s) (25 25%) 0.1240\n",
      "4m 28s (- 10m 27s) (30 30%) 0.0860\n",
      "5m 10s (- 9m 36s) (35 35%) 0.0654\n",
      "5m 57s (- 8m 56s) (40 40%) 0.0534\n",
      "6m 44s (- 8m 14s) (45 45%) 0.0457\n",
      "7m 26s (- 7m 26s) (50 50%) 0.0411\n",
      "8m 9s (- 6m 40s) (55 55%) 0.0375\n",
      "8m 52s (- 5m 54s) (60 60%) 0.0349\n",
      "9m 34s (- 5m 9s) (65 65%) 0.0328\n",
      "10m 16s (- 4m 24s) (70 70%) 0.0315\n",
      "10m 58s (- 3m 39s) (75 75%) 0.0300\n",
      "11m 47s (- 2m 56s) (80 80%) 0.0295\n",
      "12m 32s (- 2m 12s) (85 85%) 0.0282\n",
      "13m 17s (- 1m 28s) (90 90%) 0.0281\n",
      "13m 59s (- 0m 44s) (95 95%) 0.0267\n",
      "14m 42s (- 0m 0s) (100 100%) 0.0261\n",
      "> je suis suppose etre celui qui vous aide\n",
      "= i m supposed to be the one helping you\n",
      "< i m supposed to be the one helping you <EOS>\n",
      "\n",
      "> nous sommes tous infectes\n",
      "= we re all infected\n",
      "< we re all infected <EOS>\n",
      "\n",
      "> nous y allons tous\n",
      "= we re all going\n",
      "< we re all going <EOS>\n",
      "\n",
      "> elles sont a trente dollars chacune\n",
      "= they re thirty dollars each\n",
      "< they re thirty dollars each expensive <EOS>\n",
      "\n",
      "> je me rejouis de vous avoir finalement rencontres\n",
      "= i m glad to finally meet you\n",
      "< i m glad to finally meet you <EOS>\n",
      "\n",
      "> il n est pas mon genre\n",
      "= he s not my type\n",
      "< he s not my type type <EOS>\n",
      "\n",
      "> il est nouveau en ville\n",
      "= he s new in town\n",
      "< he s new in town <EOS>\n",
      "\n",
      "> je ne pleure pas\n",
      "= i m not crying\n",
      "< i m not crying <EOS>\n",
      "\n",
      "> il est reporter pour le time\n",
      "= he is a reporter for time magazine\n",
      "< he is a reporter for time magazine <EOS>\n",
      "\n",
      "> je suis libre\n",
      "= i m free\n",
      "< i am free <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder_attn = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder_attn = AttnDecoderRNN(hidden_size, output_lang.n_words, device=device).to(device)\n",
    "\n",
    "train(train_dataloader, encoder_attn, decoder_attn, 100, print_every=5, plot_every=5)\n",
    "\n",
    "encoder_attn.eval()\n",
    "decoder_attn.eval()\n",
    "evaluateRandomly(encoder_attn, decoder_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5c96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> c est un bon chanteur\n",
      "= he is a good singer\n",
      "< he is a good singer singer <EOS>\n",
      "\n",
      "> je suis fiere de mon pere\n",
      "= i m proud of my father\n",
      "< i m proud of my father s my father <EOS>\n",
      "\n",
      "> nous nous appretons justement a manger\n",
      "= we re just getting ready to eat\n",
      "< we re just getting ready to eat <EOS>\n",
      "\n",
      "> ils emmenent marie a la salle des urgences\n",
      "= they re taking mary to the emergency room\n",
      "< they re taking mary to the emergency room <EOS>\n",
      "\n",
      "> tu as completement tort\n",
      "= you are completely wrong\n",
      "< you are completely wrong <EOS>\n",
      "\n",
      "> je travaille aussi vite que je peux\n",
      "= i m working as fast as i can\n",
      "< i m working as fast as i can <EOS>\n",
      "\n",
      "> nous faisons encore des emplettes\n",
      "= we re still shopping around\n",
      "< we re still shopping around the wrong number <EOS>\n",
      "\n",
      "> je suis masochiste\n",
      "= i m a masochist\n",
      "< i am a masochist <EOS>\n",
      "\n",
      "> je ne suis vraiment pas tres bonne en francais\n",
      "= i m really not very good at french\n",
      "< i m really not very good at french <EOS>\n",
      "\n",
      "> je ne suis pas actuellement arme\n",
      "= i m now unarmed\n",
      "< i m now unarmed <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder_attn, decoder_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a18d2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "def evaluateMetrics(encoder, decoder, n=100):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        hypotheses.append(output_sentence)\n",
    "        references.append(pair[1])\n",
    "    \n",
    "    bleu = corpus_bleu(hypotheses, references)\n",
    "    return bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efc079c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_attn = evaluateMetrics(encoder_attn, decoder_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0178e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6964380312467363"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_attn.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc79c67d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bleu_rnn = evaluateMetrics(\u001b[43mencoder\u001b[49m, decoder)\n\u001b[32m      2\u001b[39m bleu_rnn.score\n",
      "\u001b[31mNameError\u001b[39m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "bleu_rnn = evaluateMetrics(encoder, decoder)\n",
    "bleu_rnn.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8878f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
